# TeyMCP-Server GPU é…ç½®æŒ‡å—

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
- [ç«¯å£è§„åˆ’](#ç«¯å£è§„åˆ’)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [GPU æœåŠ¡å¯¹æ¥](#gpu-æœåŠ¡å¯¹æ¥)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸ–¥ï¸ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- âœ… NVIDIA GPUï¼ˆæ”¯æŒ CUDA 12.3+ï¼‰
- âœ… è‡³å°‘ 8GB ç³»ç»Ÿå†…å­˜
- âœ… 20GB+ å¯ç”¨ç£ç›˜ç©ºé—´

### è½¯ä»¶è¦æ±‚
- âœ… Ubuntu 20.04+ / Debian 11+
- âœ… NVIDIA é©±åŠ¨ >= 525.60.11
- âœ… Docker >= 20.10
- âœ… Docker Compose >= 2.0

---

## ğŸš€ å®‰è£…æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ NVIDIA é©±åŠ¨

```bash
# æ£€æŸ¥é©±åŠ¨æ˜¯å¦å®‰è£…
nvidia-smi

# é¢„æœŸè¾“å‡ºåº”æ˜¾ç¤º GPU ä¿¡æ¯å’Œé©±åŠ¨ç‰ˆæœ¬
# å¦‚æœªå®‰è£…ï¼Œè¯·è®¿é—®: https://www.nvidia.com/Download/index.aspx
```

### ç¬¬äºŒæ­¥ï¼šå®‰è£… NVIDIA Container Toolkit

```bash
cd /home/sun/TeyMCP-Server

# è¿è¡Œè‡ªåŠ¨å®‰è£…è„šæœ¬
sudo bash install_nvidia_container_toolkit.sh
```

å®‰è£…è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆï¼š
1. âœ… æ£€æŸ¥ NVIDIA GPU å’Œé©±åŠ¨
2. âœ… æ£€æŸ¥/å®‰è£… Docker
3. âœ… å®‰è£… NVIDIA Container Toolkit
4. âœ… é…ç½® Docker Runtime
5. âœ… éªŒè¯ GPU å®¹å™¨è®¿é—®

### ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å®‰è£…

```bash
# æµ‹è¯• GPU å®¹å™¨è®¿é—®
docker run --rm --gpus all nvidia/cuda:12.3.0-base-ubuntu22.04 nvidia-smi

# åº”è¯¥æ˜¾ç¤º GPU ä¿¡æ¯ï¼Œè¯´æ˜å®¹å™¨å¯ä»¥è®¿é—® GPU
```

---

## ğŸ”Œ ç«¯å£è§„åˆ’

ä¸ºé¿å…ç«¯å£å†²çªï¼ŒTeyMCP-Server ä½¿ç”¨ä»¥ä¸‹ç«¯å£åˆ†é…ï¼š

| æœåŠ¡åç§° | å®¹å™¨å†…ç«¯å£ | å®¿ä¸»æœºç«¯å£ | ç”¨é€” | çŠ¶æ€ |
|---------|-----------|-----------|------|------|
| TeyMCP-Server | 8080 | **1215** | MCP èšåˆæœåŠ¡ä¸»ç«¯å£ | âœ… å¿…éœ€ |
| Ollama (å¯é€‰) | 11434 | **11434** | æœ¬åœ° LLM æ¨ç†æœåŠ¡ | ğŸ”„ å¯é€‰ |
| é¢„ç•™ç«¯å£ | - | **1216** | æœªæ¥ GPU æœåŠ¡æ‰©å±• | ğŸ”„ å¯é€‰ |

### ç«¯å£å†²çªæ£€æŸ¥

```bash
# æ£€æŸ¥ç«¯å£å ç”¨æƒ…å†µ
sudo netstat -tlnp | grep -E '1215|1216|11434'

# æˆ–ä½¿ç”¨ lsof
sudo lsof -i :1215
sudo lsof -i :1216
sudo lsof -i :11434
```

### ç«¯å£é…ç½®è¯´æ˜

**ä¸ºä»€ä¹ˆé€‰æ‹© 1215 ç«¯å£ï¼Ÿ**
- åŸæœåŠ¡ç«¯å£ 8080 å¸¸è¢«å…¶ä»–æœåŠ¡å ç”¨
- 1215 ç«¯å£èŒƒå›´è¾ƒå°‘å†²çª
- å®¹å™¨å†…ä»ä½¿ç”¨ 8080ï¼Œå¤–éƒ¨æ˜ å°„åˆ° 1215

**å¦‚ä½•ä¿®æ”¹ç«¯å£ï¼Ÿ**
ç¼–è¾‘ `docker-compose.yml`:
```yaml
ports:
  - "æ‚¨çš„ç«¯å£:8080"  # ä¿®æ”¹å†’å·å‰çš„ç«¯å£å·
```

---

## ğŸ“¦ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Docker Composeï¼ˆæ¨èï¼‰

```bash
cd /home/sun/TeyMCP-Server

# 1. æ„å»º GPU é•œåƒ
docker-compose build

# 2. å¯åŠ¨æœåŠ¡ï¼ˆGPU æ¨¡å¼ï¼‰
docker-compose up -d

# 3. æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f teymcp

# 4. æ£€æŸ¥ GPU ä½¿ç”¨æƒ…å†µ
docker exec teymcp-server-gpu nvidia-smi

# 5. åœæ­¢æœåŠ¡
docker-compose down
```

### æ–¹å¼äºŒï¼šç›´æ¥ä½¿ç”¨ Docker

```bash
cd /home/sun/TeyMCP-Server

# æ„å»ºé•œåƒ
docker build -t teymcp-server:gpu-latest .

# è¿è¡Œå®¹å™¨ï¼ˆå•ç‹¬è¿è¡Œï¼‰
docker run -d \
  --name teymcp-server-gpu \
  --gpus all \
  -p 1215:8080 \
  -v $(pwd)/config:/app/config:ro \
  -v teymcp-logs:/app/data/logs \
  -v teymcp-metrics:/app/data/metrics \
  -e NVIDIA_VISIBLE_DEVICES=all \
  --restart unless-stopped \
  teymcp-server:gpu-latest

# æŸ¥çœ‹æ—¥å¿—
docker logs -f teymcp-server-gpu

# åœæ­¢å®¹å™¨
docker stop teymcp-server-gpu
docker rm teymcp-server-gpu
```

### æ–¹å¼ä¸‰ï¼šå¯ç”¨ Ollama æœ¬åœ° LLMï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦æœ¬åœ° GPU æ¨ç†æœåŠ¡ï¼š

```bash
# 1. ç¼–è¾‘ docker-compose.ymlï¼Œå–æ¶ˆ Ollama éƒ¨åˆ†çš„æ³¨é‡Š
vim docker-compose.yml
# æ‰¾åˆ° ollama æœåŠ¡å®šä¹‰ï¼Œåˆ é™¤å‰é¢çš„ # æ³¨é‡Šç¬¦å·

# 2. å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# 3. éªŒè¯ Ollama
curl http://localhost:11434/api/version

# 4. æ‹‰å–æ¨¡å‹ï¼ˆä¾‹å¦‚ llama2ï¼‰
docker exec -it ollama-gpu ollama pull llama2

# 5. æµ‹è¯•æ¨ç†
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, how are you?"
}'
```

---

## ğŸ”— GPU æœåŠ¡å¯¹æ¥

### å¯¹æ¥æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å®¿ä¸»æœºï¼ˆHostï¼‰                    â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Docker Network (teymcp-network)       â”‚ â”‚
â”‚  â”‚                                               â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚  TeyMCP-Server      â”‚  â”‚  Ollama (å¯é€‰) â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  Container          â”‚  â”‚  Container     â”‚ â”‚ â”‚
â”‚  â”‚  â”‚                     â”‚  â”‚                â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  Port: 8080â†’1215    â”‚  â”‚  Port: 11434  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  GPU: âœ… Enabled    â”‚  â”‚  GPU: âœ… All   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚                     â”‚  â”‚                â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  MCP Servers:       â”‚â—„â”€â”¤  API Endpoint  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  - Ollama MCP       â”‚  â”‚                â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  - HuggingFace      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚  â”‚  - DeepSeek         â”‚                     â”‚ â”‚
â”‚  â”‚  â”‚  - ...å…¶ä»–66ä¸ª       â”‚                     â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚ â”‚
â”‚  â”‚           â–²                                   â”‚ â”‚
â”‚  â”‚           â”‚ GPU Access                        â”‚ â”‚
â”‚  â”‚           â–¼                                   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚ â”‚
â”‚  â”‚  â”‚   NVIDIA GPU        â”‚                     â”‚ â”‚
â”‚  â”‚  â”‚   (via nvidia-ctk)  â”‚                     â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  External Access: http://YOUR_IP:1215               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. TeyMCP-Server å†…éƒ¨å¯¹æ¥

TeyMCP-Server å¯ä»¥é€šè¿‡ MCP åè®®è°ƒç”¨ GPU åŠ é€Ÿçš„æœåŠ¡ï¼š

#### é…ç½® Ollama MCP Server

ç¼–è¾‘ `config/servers.yaml`:

```yaml
servers:
  # Ollama æœ¬åœ° LLMï¼ˆGPU åŠ é€Ÿï¼‰
  ollama:
    enabled: true  # å¯ç”¨
    command: "npx"
    args:
      - "-y"
      - "ollama-mcp-server"
    env:
      OLLAMA_HOST: "http://ollama:11434"  # Docker ç½‘ç»œå†…è®¿é—®
      # æˆ–å¦‚æœ Ollama åœ¨å®¿ä¸»æœº: "http://host.docker.internal:11434"
```

#### é…ç½®å…¶ä»– GPU æœåŠ¡

```yaml
  # DeepSeek MCPï¼ˆGPU æ¨ç†ï¼‰
  deepseek_mcp:
    enabled: true
    command: "npx"
    args:
      - "-y"
      - "deepseek-mcp-server"
    env:
      DEEPSEEK_API_KEY: "${DEEPSEEK_API_KEY}"
      USE_GPU: "true"

  # HuggingFaceï¼ˆGPU æ¨ç†ï¼‰
  huggingface_official:
    enabled: true
    command: "npx"
    args:
      - "-y"
      - "@llmindset/hf-mcp-server"
    env:
      HUGGINGFACE_TOKEN: "${HUGGINGFACE_TOKEN}"
      DEVICE: "cuda"  # ä½¿ç”¨ GPU
```

### 2. å¤–éƒ¨åº”ç”¨å¯¹æ¥

#### Claude Desktop é…ç½®

ç¼–è¾‘ `~/.config/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "teymcp-gpu": {
      "command": "curl",
      "args": [
        "-X", "POST",
        "http://YOUR_SERVER_IP:1215/api/tools/call",
        "-H", "Content-Type: application/json",
        "-d", "{\"server\":\"ollama\",\"tool\":\"generate\",\"params\":{\"prompt\":\"YOUR_PROMPT\"}}"
      ]
    }
  }
}
```

#### Python SDK å¯¹æ¥

```python
import requests

class TeyMCPClient:
    def __init__(self, base_url="http://localhost:1215"):
        self.base_url = base_url
    
    def call_gpu_tool(self, server, tool, params):
        """è°ƒç”¨ GPU åŠ é€Ÿçš„å·¥å…·"""
        response = requests.post(
            f"{self.base_url}/api/tools/call",
            json={
                "server": server,
                "tool": tool,
                "params": params
            }
        )
        return response.json()
    
    def ollama_generate(self, prompt, model="llama2"):
        """ä½¿ç”¨ Ollama GPU æ¨ç†"""
        return self.call_gpu_tool(
            server="ollama",
            tool="generate",
            params={
                "model": model,
                "prompt": prompt
            }
        )

# ä½¿ç”¨ç¤ºä¾‹
client = TeyMCPClient("http://YOUR_IP:1215")
result = client.ollama_generate("What is AI?")
print(result)
```

#### curl å‘½ä»¤å¯¹æ¥

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å·¥å…·
curl http://localhost:1215/api/tools | jq

# è°ƒç”¨ Ollama ç”Ÿæˆ
curl -X POST http://localhost:1215/api/tools/call \
  -H "Content-Type: application/json" \
  -d '{
    "server": "ollama",
    "tool": "generate",
    "params": {
      "model": "llama2",
      "prompt": "Explain quantum computing"
    }
  }' | jq

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:1215/api/status | jq
```

### 3. å®¹å™¨é—´é€šä¿¡

åœ¨ Docker Compose ç¯å¢ƒä¸­ï¼ŒæœåŠ¡å¯ä»¥ç›´æ¥é€šè¿‡æœåŠ¡åè®¿é—®ï¼š

```yaml
# TeyMCP-Server è®¿é—® Ollama
OLLAMA_HOST: "http://ollama:11434"

# è€Œä¸æ˜¯
# OLLAMA_HOST: "http://localhost:11434"
```

å¦‚æœéœ€è¦ä»å®¹å™¨è®¿é—®å®¿ä¸»æœºæœåŠ¡ï¼š
```yaml
environment:
  - EXTERNAL_SERVICE: "http://host.docker.internal:PORT"
```

---

## ğŸ” éªŒè¯å’Œæµ‹è¯•

### åŸºç¡€éªŒè¯

```bash
# 1. æ£€æŸ¥å®¹å™¨è¿è¡ŒçŠ¶æ€
docker ps | grep teymcp

# 2. æ£€æŸ¥ GPU å¯è§æ€§
docker exec teymcp-server-gpu nvidia-smi

# 3. æ£€æŸ¥æœåŠ¡å¥åº·
curl http://localhost:1215/health

# 4. æŸ¥çœ‹æœåŠ¡çŠ¶æ€
curl http://localhost:1215/api/status | jq

# 5. æŸ¥çœ‹æ—¥å¿—
docker logs -f teymcp-server-gpu
```

### GPU åŠŸèƒ½æµ‹è¯•

```bash
# æµ‹è¯• CUDA æ˜¯å¦å¯ç”¨
docker exec teymcp-server-gpu python3 -c "
import torch
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'CUDA Device: {torch.cuda.get_device_name(0)}')
"

# æµ‹è¯• GPU å†…å­˜
docker exec teymcp-server-gpu nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv
```

### MCP å·¥å…·æµ‹è¯•

```bash
# æŸ¥çœ‹æ‰€æœ‰ MCP å·¥å…·
curl http://localhost:1215/api/tools | jq '.[] | select(.server == "ollama")'

# æµ‹è¯• Ollama å·¥å…·è°ƒç”¨
curl -X POST http://localhost:1215/api/tools/call \
  -H "Content-Type: application/json" \
  -d '{
    "server": "ollama",
    "tool": "list_models",
    "params": {}
  }' | jq
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: GPU ä¸å¯è§

**ç—‡çŠ¶**: `nvidia-smi` åœ¨å®¹å™¨å†…æ— æ³•è¿è¡Œ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ NVIDIA Container Toolkit
sudo systemctl status docker
sudo nvidia-ctk --version

# é‡æ–°é…ç½® Docker Runtime
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# é‡æ–°æ„å»ºå®¹å™¨
docker-compose down
docker-compose up -d
```

### é—®é¢˜ 2: ç«¯å£å†²çª

**ç—‡çŠ¶**: `Error starting userland proxy: listen tcp4 0.0.0.0:1215: bind: address already in use`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
sudo lsof -i :1215
sudo netstat -tlnp | grep 1215

# æ€æ­»å ç”¨è¿›ç¨‹
sudo kill -9 <PID>

# æˆ–ä¿®æ”¹ docker-compose.yml ä½¿ç”¨å…¶ä»–ç«¯å£
vim docker-compose.yml
# ä¿®æ”¹ ports: "1215:8080" ä¸º "YOUR_PORT:8080"
```

### é—®é¢˜ 3: GPU å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥çœ‹ GPU å†…å­˜ä½¿ç”¨
nvidia-smi

# é™åˆ¶ GPU å†…å­˜ï¼ˆä¿®æ”¹ docker-compose.ymlï¼‰
environment:
  - CUDA_VISIBLE_DEVICES=0  # åªä½¿ç”¨ç¬¬ä¸€ä¸ª GPU
  # æˆ–åœ¨åº”ç”¨å±‚é¢é™åˆ¶
  - TF_FORCE_GPU_ALLOW_GROWTH=true
```

### é—®é¢˜ 4: Ollama æ— æ³•è¿æ¥

**ç—‡çŠ¶**: Connection refused to ollama:11434

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ Ollama å®¹å™¨çŠ¶æ€
docker ps | grep ollama

# æ£€æŸ¥ç½‘ç»œè¿æ¥
docker exec teymcp-server-gpu ping ollama

# æŸ¥çœ‹ Ollama æ—¥å¿—
docker logs ollama-gpu

# æµ‹è¯• Ollama API
curl http://localhost:11434/api/version
```

### é—®é¢˜ 5: æƒé™é—®é¢˜

**ç—‡çŠ¶**: Permission denied accessing GPU

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ·»åŠ å½“å‰ç”¨æˆ·åˆ° docker ç»„
sudo usermod -aG docker $USER

# é‡æ–°ç™»å½•æˆ–è¿è¡Œ
newgrp docker

# æˆ–ä¸´æ—¶ä½¿ç”¨ sudo
sudo docker-compose up -d
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### å®æ—¶ç›‘æ§è„šæœ¬

åˆ›å»º `monitor_gpu.sh`:

```bash
#!/bin/bash
watch -n 1 '
echo "=== GPU Status ==="
nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.used,memory.free --format=csv,noheader,nounits

echo ""
echo "=== Container Stats ==="
docker stats --no-stream teymcp-server-gpu

echo ""
echo "=== TeyMCP Status ==="
curl -s http://localhost:1215/api/status | jq -r ".servers | to_entries[] | select(.value.enabled) | \"\(.key): \(.value.status)\""
'
```

ä½¿ç”¨:
```bash
chmod +x monitor_gpu.sh
./monitor_gpu.sh
```

---

## ğŸ” å®‰å…¨å»ºè®®

1. **ä¸è¦ä½¿ç”¨ privileged æ¨¡å¼**: å·²åœ¨ docker-compose.yml ä¸­ç¦ç”¨
2. **é™åˆ¶ GPU è®¿é—®**: å¯ä»¥ä½¿ç”¨ `count: 1` é™åˆ¶ GPU æ•°é‡
3. **ä½¿ç”¨åªè¯»æŒ‚è½½**: é…ç½®æ–‡ä»¶ä½¿ç”¨ `:ro` æ ‡å¿—
4. **é…ç½®é˜²ç«å¢™**: é™åˆ¶ 1215 ç«¯å£çš„è®¿é—®
```bash
sudo ufw allow from YOUR_TRUSTED_IP to any port 1215
```
5. **å®šæœŸæ›´æ–°**: ä¿æŒ NVIDIA é©±åŠ¨å’Œ Container Toolkit æœ€æ–°

---

## ğŸ“š æ›´å¤šèµ„æº

- **NVIDIA Container Toolkit æ–‡æ¡£**: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/
- **Docker GPU æ”¯æŒ**: https://docs.docker.com/config/containers/resource_constraints/#gpu
- **Ollama æ–‡æ¡£**: https://github.com/ollama/ollama
- **TeyMCP-Server ä¸»æ–‡æ¡£**: [README.md](../README.md)

---

## âœ… å¿«é€Ÿå¯åŠ¨æ¸…å•

- [ ] å®‰è£… NVIDIA é©±åŠ¨
- [ ] è¿è¡Œ `install_nvidia_container_toolkit.sh`
- [ ] éªŒè¯ GPU å®¹å™¨è®¿é—®
- [ ] æ£€æŸ¥ç«¯å£ 1215 æ˜¯å¦å¯ç”¨
- [ ] é…ç½® `config/servers.yaml` å¯ç”¨ GPU æœåŠ¡
- [ ] é…ç½® `config/.env` æ·»åŠ  API å¯†é’¥
- [ ] è¿è¡Œ `docker-compose up -d`
- [ ] éªŒè¯æœåŠ¡: `curl http://localhost:1215/api/status`
- [ ] æµ‹è¯• GPU: `docker exec teymcp-server-gpu nvidia-smi`
- [ ] æµ‹è¯• MCP å·¥å…·è°ƒç”¨

---

**ğŸ‰ æ­å–œï¼æ‚¨çš„ TeyMCP-Server ç°åœ¨å·²ç»æ”¯æŒ GPU åŠ é€Ÿäº†ï¼**
