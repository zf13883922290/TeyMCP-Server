# TeyMCP-Server + NVIDIA GPU å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ ä¸‰æ­¥å¿«é€Ÿå¯åŠ¨

### æ­¥éª¤ 1: å®‰è£… NVIDIA Container Toolkit

```bash
cd /home/sun/TeyMCP-Server
sudo bash install_nvidia_container_toolkit.sh
```

è¿™å°†è‡ªåŠ¨å®Œæˆï¼š
- âœ… æ£€æŸ¥ NVIDIA GPU å’Œé©±åŠ¨
- âœ… å®‰è£… Dockerï¼ˆå¦‚æœªå®‰è£…ï¼‰
- âœ… å®‰è£… NVIDIA Container Toolkit
- âœ… é…ç½® Docker Runtime
- âœ… éªŒè¯ GPU å®¹å™¨è®¿é—®

### æ­¥éª¤ 2: å¯åŠ¨ GPU æœåŠ¡

```bash
bash start_gpu.sh
```

è¿™å°†è‡ªåŠ¨å®Œæˆï¼š
- âœ… æ£€æŸ¥ GPU å’Œå·¥å…·é“¾
- âœ… æ£€æŸ¥ç«¯å£å†²çª
- âœ… æ„å»º GPU é•œåƒ
- âœ… å¯åŠ¨æœåŠ¡
- âœ… éªŒè¯æœåŠ¡å°±ç»ª

### æ­¥éª¤ 3: éªŒè¯å’Œæµ‹è¯•

```bash
bash test_gpu.sh
```

è¿™å°†æµ‹è¯•ï¼š
- âœ… Docker å’Œå®¹å™¨çŠ¶æ€
- âœ… GPU è®¿é—®å’ŒåŠŸèƒ½
- âœ… MCP æœåŠ¡å’Œ API
- âœ… ç«¯å£å’Œæ—¥å¿—

---

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### å®æ—¶ç›‘æ§

```bash
bash monitor_gpu.sh
```

æ˜¾ç¤ºï¼š
- ğŸ–¥ï¸ GPU çŠ¶æ€ï¼ˆæ¸©åº¦ã€åŠŸè€—ã€æ˜¾å­˜ï¼‰
- ğŸ“¦ å®¹å™¨èµ„æºä½¿ç”¨
- ğŸ”§ MCP æœåŠ¡çŠ¶æ€
- ğŸ“ˆ å·¥å…·ç»Ÿè®¡
- ğŸ“ å®æ—¶æ—¥å¿—
- ğŸ–¥ï¸ ç³»ç»Ÿèµ„æº

### æŸ¥çœ‹æ—¥å¿—

```bash
# å®æ—¶æ—¥å¿—
docker-compose logs -f

# åªçœ‹ä¸»æœåŠ¡
docker-compose logs -f teymcp

# æœ€è¿‘ 100 è¡Œ
docker logs --tail 100 teymcp-server-gpu
```

### åœæ­¢æœåŠ¡

```bash
docker-compose down
```

### é‡å¯æœåŠ¡

```bash
docker-compose restart
```

---

## ğŸ”Œ ç«¯å£å’Œè®¿é—®

| ç«¯å£ | æœåŠ¡ | è®¿é—®åœ°å€ |
|------|------|----------|
| 1215 | TeyMCP-Server ä¸»æœåŠ¡ | http://localhost:1215 |
| 1215 | API æ–‡æ¡£ | http://localhost:1215/docs |
| 1215 | å¥åº·æ£€æŸ¥ | http://localhost:1215/health |
| 1215 | æœåŠ¡çŠ¶æ€ | http://localhost:1215/api/status |
| 11434 | Ollama (å¯é€‰) | http://localhost:11434 |

### æµ‹è¯•è¿æ¥

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:1215/health

# æœåŠ¡çŠ¶æ€
curl http://localhost:1215/api/status | jq

# å·¥å…·åˆ—è¡¨
curl http://localhost:1215/api/tools | jq

# GPU çŠ¶æ€
docker exec teymcp-server-gpu nvidia-smi
```

---

## ğŸ”§ é…ç½® GPU æœåŠ¡

### 1. å¯ç”¨ Ollama æœ¬åœ° LLM

ç¼–è¾‘ `docker-compose.yml`ï¼Œå–æ¶ˆ Ollama éƒ¨åˆ†çš„æ³¨é‡Šï¼š

```bash
vim docker-compose.yml
# æ‰¾åˆ° "# ollama:" å¼€å§‹çš„éƒ¨åˆ†ï¼Œåˆ é™¤æ‰€æœ‰ # æ³¨é‡Šç¬¦å·
```

ç„¶åé‡å¯ï¼š

```bash
docker-compose up -d
```

æ‹‰å–æ¨¡å‹ï¼š

```bash
docker exec ollama-gpu ollama pull llama2
docker exec ollama-gpu ollama pull codellama
```

æµ‹è¯•ï¼š

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, how are you?"
}'
```

### 2. é…ç½® MCP æœåŠ¡å™¨ä½¿ç”¨ GPU

ç¼–è¾‘ `config/servers.yaml`:

```yaml
servers:
  # Ollama MCPï¼ˆè¿æ¥åˆ°å®¹å™¨å†… Ollamaï¼‰
  ollama:
    enabled: true
    command: "npx"
    args:
      - "-y"
      - "ollama-mcp-server"
    env:
      OLLAMA_HOST: "http://ollama:11434"  # Docker ç½‘ç»œå†…è®¿é—®
```

æ·»åŠ  API å¯†é’¥åˆ° `config/.env`:

```bash
# HuggingFaceï¼ˆä½¿ç”¨ GPUï¼‰
HUGGINGFACE_TOKEN=your_token_here

# DeepSeek
DEEPSEEK_API_KEY=your_api_key_here

# Ollamaï¼ˆæœ¬åœ°ï¼Œæ— éœ€å¯†é’¥ï¼‰
OLLAMA_HOST=http://ollama:11434
```

é‡å¯æœåŠ¡ï¼š

```bash
docker-compose restart
```

---

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### Python å®¢æˆ·ç«¯

```python
import requests

# åˆå§‹åŒ–å®¢æˆ·ç«¯
BASE_URL = "http://localhost:1215"

# 1. æŸ¥çœ‹æœåŠ¡çŠ¶æ€
response = requests.get(f"{BASE_URL}/api/status")
print(response.json())

# 2. æŸ¥çœ‹æ‰€æœ‰å·¥å…·
response = requests.get(f"{BASE_URL}/api/tools")
tools = response.json()
print(f"æ€»å·¥å…·æ•°: {len(tools)}")

# 3. è°ƒç”¨ Ollama ç”Ÿæˆæ–‡æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
response = requests.post(
    f"{BASE_URL}/api/tools/call",
    json={
        "server": "ollama",
        "tool": "generate",
        "params": {
            "model": "llama2",
            "prompt": "Explain quantum computing in simple terms"
        }
    }
)
print(response.json())

# 4. ä½¿ç”¨ GitHub å·¥å…·
response = requests.post(
    f"{BASE_URL}/api/tools/call",
    json={
        "server": "github",
        "tool": "search_repositories",
        "params": {
            "query": "machine learning",
            "limit": 5
        }
    }
)
print(response.json())
```

### curl å‘½ä»¤

```bash
# æŸ¥çœ‹çŠ¶æ€
curl http://localhost:1215/api/status | jq '.servers | keys'

# æŸ¥çœ‹ GPU ç›¸å…³å·¥å…·
curl http://localhost:1215/api/tools | jq '.[] | select(.server | contains("ollama") or contains("huggingface"))'

# è°ƒç”¨å·¥å…·
curl -X POST http://localhost:1215/api/tools/call \
  -H "Content-Type: application/json" \
  -d '{
    "server": "ollama",
    "tool": "list_models",
    "params": {}
  }' | jq
```

### Claude Desktop é…ç½®

ç¼–è¾‘ `~/.config/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "teymcp": {
      "url": "http://YOUR_SERVER_IP:1215",
      "headers": {
        "Content-Type": "application/json"
      }
    }
  }
}
```

---

## ğŸ› å¸¸è§é—®é¢˜

### GPU ä¸å¯è§

```bash
# æ£€æŸ¥é©±åŠ¨
nvidia-smi

# é‡æ–°é…ç½®
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# é‡å¯å®¹å™¨
docker-compose restart
```

### ç«¯å£å†²çª

```bash
# æ£€æŸ¥å ç”¨
sudo lsof -i :1215

# æ€æ­»è¿›ç¨‹
sudo kill -9 <PID>

# æˆ–ä¿®æ”¹ç«¯å£
vim docker-compose.yml
# ä¿®æ”¹ ports: "YOUR_PORT:8080"
```

### æœåŠ¡æ— å“åº”

```bash
# æŸ¥çœ‹æ—¥å¿—
docker logs teymcp-server-gpu

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps -a | grep teymcp

# é‡å¯æœåŠ¡
docker-compose restart
```

### GPU å†…å­˜ä¸è¶³

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨
nvidia-smi

# é™åˆ¶ä½¿ç”¨çš„ GPU
vim docker-compose.yml
# ä¿®æ”¹ count: 1  # åªä½¿ç”¨ä¸€ä¸ª GPU

# æ¸…ç†ä¸ç”¨çš„å®¹å™¨
docker system prune -a
```

---

## ğŸ“š è„šæœ¬è¯´æ˜

| è„šæœ¬ | ç”¨é€” | ä½•æ—¶ä½¿ç”¨ |
|------|------|----------|
| `install_nvidia_container_toolkit.sh` | å®‰è£… NVIDIA å·¥å…· | é¦–æ¬¡å®‰è£… |
| `start_gpu.sh` | å¯åŠ¨ GPU æœåŠ¡ | æ¯æ¬¡å¯åŠ¨ |
| `test_gpu.sh` | æµ‹è¯• GPU åŠŸèƒ½ | éªŒè¯é…ç½® |
| `monitor_gpu.sh` | å®æ—¶ç›‘æ§ | æ—¥å¸¸ç›‘æ§ |

---

## ğŸ”— æ›´å¤šèµ„æº

- ğŸ“– å®Œæ•´æ–‡æ¡£: `docs/GPU_SETUP.md`
- ğŸ“Š æ›´æ–°æ€»ç»“: `UPDATE_SUMMARY_20251105.md`
- ğŸ”§ é…ç½®æ–‡ä»¶: `config/servers.yaml`
- ğŸŒ API æ–‡æ¡£: http://localhost:1215/docs

---

## âœ… å¿«é€Ÿæ£€æŸ¥æ¸…å•

ä½¿ç”¨å‰è¯·ç¡®è®¤ï¼š

- [ ] NVIDIA é©±åŠ¨å·²å®‰è£…ï¼ˆ`nvidia-smi` å¯ç”¨ï¼‰
- [ ] Docker å·²å®‰è£…å¹¶è¿è¡Œ
- [ ] NVIDIA Container Toolkit å·²å®‰è£…
- [ ] ç«¯å£ 1215 å¯ç”¨
- [ ] é…ç½®æ–‡ä»¶ `config/servers.yaml` å­˜åœ¨
- [ ] ç¯å¢ƒå˜é‡ `config/.env` å·²é…ç½®ï¼ˆå¦‚éœ€è¦ï¼‰

---

**ğŸ‰ å¼€å§‹ä½¿ç”¨ TeyMCP-Server + GPUï¼**

```bash
# ä¸€é”®å¯åŠ¨
bash start_gpu.sh

# ä¸€é”®æµ‹è¯•
bash test_gpu.sh

# ä¸€é”®ç›‘æ§
bash monitor_gpu.sh
```
